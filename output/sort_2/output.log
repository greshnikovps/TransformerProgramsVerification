I 2025-07-31T17:01:11 __main__:675: args: {'output_dir': 'output/sort_2', 'dataset': 'sort', 'vocab_size': 8, 'dataset_size': 10000, 'min_length': 1, 'max_length': 8, 'seed': 0, 'do_lower': 0, 'unique': 1, 'replace_numbers': 0, 'n_vars_cat': 1, 'n_vars_num': 1, 'd_var': None, 'n_heads_cat': 4, 'n_heads_num': 0, 'd_mlp': 64, 'n_cat_mlps': 1, 'n_num_mlps': 0, 'mlp_vars_in': 2, 'n_layers': 2, 'sample_fn': 'gumbel_soft', 'one_hot_embed': True, 'count_only': True, 'selector_width': 0, 'attention_type': 'cat', 'rel_pos_bias': 'fixed', 'mlp_type': 'cat', 'autoregressive': False, 'glove_embeddings': 'data/glove.840B.300d.txt', 'do_glove': 0, 'unembed_mask': 1, 'pool_outputs': 0, 'standard': False, 'd_model': 64, 'd_head': None, 'n_heads': 2, 'dropout': 0.0, 'lr': 0.05, 'max_grad_norm': None, 'gumbel_samples': 1, 'n_epochs': 250, 'batch_size': 512, 'tau_init': 3.0, 'tau_end': 0.01, 'tau_schedule': 'geomspace', 'loss_agg': 'per_token', 'save': True, 'save_code': True, 'save_Z3': True, 'device': 'cpu'}
I 2025-07-31T17:01:14 __main__:643: vocab size: 8
I 2025-07-31T17:01:14 __main__:644: X_train: (8100, 8), Y_train, (8100, 8)
I 2025-07-31T17:01:14 __main__:645: X_val: (900, 8), Y_val, (900, 8)
I 2025-07-31T17:01:14 __main__:646: X_test: (1000, 8), Y_test, (1000, 8)
I 2025-07-31T17:01:14 __main__:649: 8100/8100 unique training inputs
I 2025-07-31T17:01:14 __main__:650: 1000/1000 unique test inputs not in train
I 2025-07-31T17:14:20 __main__:452: train: loss=0.039420899003744125, acc=0.9840182138791544, metrics={}
I 2025-07-31T17:14:20 __main__:452: val: loss=0.04177643358707428, acc=0.9825790949138967, metrics={}
I 2025-07-31T17:14:20 __main__:452: test: loss=0.04444409906864166, acc=0.9806034482758621, metrics={}
I 2025-07-31T17:14:20 __main__:469: saving model to output\sort_2\model.pt
I 2025-07-31T17:14:20 __main__:473: saving code to output/sort_2
I 2025-07-31T17:14:20 __main__:494: saving Z3 to output/sort_2
I 2025-07-31T17:14:20 __main__:669: writing results to output\sort_2\results.csv
